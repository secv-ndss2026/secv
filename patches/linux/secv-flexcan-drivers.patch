diff --git a/arch/arm/include/asm/smp.h b/arch/arm/include/asm/smp.h
index 8c05a7f374d8..03e9c8d58270 100644
--- a/arch/arm/include/asm/smp.h
+++ b/arch/arm/include/asm/smp.h
@@ -69,6 +69,7 @@ static inline void __cpu_die(unsigned int cpu) { }
 extern void arch_send_call_function_single_ipi(int cpu);
 extern void arch_send_call_function_ipi_mask(const struct cpumask *mask);
 extern void arch_send_wakeup_ipi_mask(const struct cpumask *mask);
+extern void arch_set_secv_ipi_handler(void(*handler)(void*));
 
 extern int register_ipi_completion(struct completion *completion, int cpu);
 
diff --git a/arch/arm/kernel/smp.c b/arch/arm/kernel/smp.c
index 3431c0553f45..14831cf35ff5 100644
--- a/arch/arm/kernel/smp.c
+++ b/arch/arm/kernel/smp.c
@@ -65,6 +65,7 @@ enum ipi_msg_type {
 	IPI_CPU_STOP,
 	IPI_IRQ_WORK,
 	IPI_COMPLETION,
+	IPI_SECV,
 	NR_IPI,
 	/*
 	 * CPU_BACKTRACE is special and not included in NR_IPI
@@ -539,6 +540,7 @@ static const char *ipi_types[NR_IPI] __tracepoint_string = {
 	[IPI_CPU_STOP]		= "CPU stop interrupts",
 	[IPI_IRQ_WORK]		= "IRQ work interrupts",
 	[IPI_COMPLETION]	= "completion interrupts",
+	[IPI_SECV]			= "SECV interrupt from SSMe",
 };
 
 static void smp_cross_call(const struct cpumask *target, unsigned int ipinr);
diff --git a/arch/arm64/boot/dts/freescale/s32g.dtsi b/arch/arm64/boot/dts/freescale/s32g.dtsi
index b81ae52f59d0..1b675c94da47 100644
--- a/arch/arm64/boot/dts/freescale/s32g.dtsi
+++ b/arch/arm64/boot/dts/freescale/s32g.dtsi
@@ -46,6 +46,22 @@ pfe_reserved_bdr: pfebufs@835e0000 {
 			/* 128 KB */
 			reg = <0 0x835e0000 0 0x20000>;
 		};
+
+		secv_shmem: sorhta_shmem@E0000000 {
+			compatible = "shared-dma-pool";
+			reg = <0 0xE0000000 0 0x2010000>;
+			no-map;
+			//reusable
+		};
+
+		secv_buffers_shmem: sorhta_buffers_shmem@E2010000 {
+			compatible = "shared-dma-pool";
+			reg = <0 0xE2010000 0 0xFF0000>;
+			//no-map;
+			reusable;
+		};
+
+
 		/* ends 0x83600000 */
 	};
 
diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index 9b31e6d0da17..892dcad51bd3 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -87,6 +87,7 @@ extern void secondary_entry(void);
 
 extern void arch_send_call_function_single_ipi(int cpu);
 extern void arch_send_call_function_ipi_mask(const struct cpumask *mask);
+extern void arch_set_secv_ipi_handler(void(*handler)(void*));
 
 #ifdef CONFIG_ARM64_ACPI_PARKING_PROTOCOL
 extern void arch_send_wakeup_ipi_mask(const struct cpumask *mask);
diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 14365ef84244..47c749cfb9cd 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -73,6 +73,7 @@ enum ipi_msg_type {
 	IPI_TIMER,
 	IPI_IRQ_WORK,
 	IPI_WAKEUP,
+	IPI_SECV,
 	NR_IPI
 };
 
@@ -799,6 +800,13 @@ void arch_send_call_function_single_ipi(int cpu)
 	smp_cross_call(cpumask_of(cpu), IPI_CALL_FUNC);
 }
 
+void(*secv_ipi_handler)(void*) = NULL;
+
+void arch_set_secv_ipi_handler(void(*handler)(void*))
+{
+	secv_ipi_handler = handler;
+}
+
 #ifdef CONFIG_ARM64_ACPI_PARKING_PROTOCOL
 void arch_send_wakeup_ipi_mask(const struct cpumask *mask)
 {
@@ -886,6 +894,13 @@ static void do_handle_IPI(int ipinr)
 			unreachable();
 		}
 		break;
+	case IPI_SECV:
+		if(secv_ipi_handler != NULL) {
+			secv_ipi_handler(NULL);
+		} else {
+			pr_crit("CPU%u: SECV IPI handler not set 0x%x\n", cpu, ipinr);
+		}
+		break;
 
 #ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
 	case IPI_TIMER:
diff --git a/drivers/net/can/dev/rx-offload.c b/drivers/net/can/dev/rx-offload.c
index 77091f7d1fa7..62cc7382aa80 100644
--- a/drivers/net/can/dev/rx-offload.c
+++ b/drivers/net/can/dev/rx-offload.c
@@ -172,6 +172,43 @@ can_rx_offload_offload_one(struct can_rx_offload *offload, unsigned int n)
 	return skb;
 }
 
+/**
+ * SECV: offload one message from the given mb. similar to the 
+ * function above, only the mailbox read function is different.
+ */
+static struct sk_buff* 
+secv_can_rx_offload_offload_one(struct can_rx_offload *offload, void* mb)
+{
+	struct sk_buff *skb;
+	struct can_rx_offload_cb *cb;
+	bool drop = false;
+	u32 timestamp;
+
+	/* If queue is full drop frame */
+	if (unlikely(skb_queue_len(&offload->skb_queue) >
+		     offload->skb_queue_len_max))
+		drop = true;
+
+	skb = offload->secv_mailbox_read(offload, mb, &timestamp, drop);
+
+		/* There was a problem reading the mailbox, propagate
+	 * error value.
+	 */
+	if (IS_ERR(skb)) {
+		//printk(KERN_ALERT "Problem reading the mailbox?\n");
+		offload->dev->stats.rx_dropped++;
+		offload->dev->stats.rx_fifo_errors++;
+
+		return skb;
+	}
+
+	/* Mailbox was read. */
+	cb = can_rx_offload_get_cb(skb);
+	cb->timestamp = timestamp;
+
+	return skb;
+}
+
 int can_rx_offload_irq_offload_timestamp(struct can_rx_offload *offload,
 					 u64 pending)
 {
@@ -199,6 +236,26 @@ int can_rx_offload_irq_offload_timestamp(struct can_rx_offload *offload,
 }
 EXPORT_SYMBOL_GPL(can_rx_offload_irq_offload_timestamp);
 
+int secv_can_rx_offload_irq_offload_timestamp(struct can_rx_offload *offload, void* secv_rx_mb) {
+	unsigned int i;
+	int received = 0;
+
+	struct sk_buff* skb;
+	skb = secv_can_rx_offload_offload_one(offload, secv_rx_mb);
+
+	if(IS_ERR_OR_NULL(skb)) {
+		//printk(KERN_ALERT "Nothing received\n");
+		return received;
+	}
+
+	//printk(KERN_ALERT "Queueing CAN message\n");
+	__skb_queue_add_sort(&offload->skb_irq_queue, skb, can_rx_offload_compare);
+	received++;
+
+	return received;
+}
+EXPORT_SYMBOL_GPL(secv_can_rx_offload_irq_offload_timestamp);
+
 int can_rx_offload_irq_offload_fifo(struct can_rx_offload *offload)
 {
 	struct sk_buff *skb;
diff --git a/drivers/net/can/dev/skb.c b/drivers/net/can/dev/skb.c
index 3ebd4f779b9b..83334f1cd904 100644
--- a/drivers/net/can/dev/skb.c
+++ b/drivers/net/can/dev/skb.c
@@ -367,6 +367,7 @@ bool can_dropped_invalid_skb(struct net_device *dev, struct sk_buff *skb)
 	return false;
 
 inval_skb:
+	printk(KERN_ALERT "Invalid CAN skb\n");
 	kfree_skb(skb);
 	dev->stats.tx_dropped++;
 	return true;
diff --git a/drivers/net/can/flexcan/flexcan-core.c b/drivers/net/can/flexcan/flexcan-core.c
index 2800e4ef6cd0..e9e2fe3ac184 100644
--- a/drivers/net/can/flexcan/flexcan-core.c
+++ b/drivers/net/can/flexcan/flexcan-core.c
@@ -31,9 +31,42 @@
 #include <linux/pm_runtime.h>
 #include <linux/regmap.h>
 #include <linux/regulator/consumer.h>
+#include <asm/barrier.h>
 
 #include "flexcan.h"
 
+// SECV: Includes
+#include <linux/of_reserved_mem.h>
+#include <linux/of_address.h>
+#include <linux/dma-mapping.h>
+#include <linux/timer.h>
+#include <linux/smp.h>
+#include "secv_flexcan.h"
+
+struct secv_can_buffer_rec {
+	void* buffer; // the pointer to the buffer base.
+	void* kernel_addr; // the kernel address base of this page.
+	u_int64_t phys_addr;
+	u_int64_t next_bit;
+	u_int64_t available; // tacks used frames
+	spinlock_t lock;
+};
+
+void __iomem *secv_shmem_virt_addr = NULL;
+struct secv_flexcan_priv *secv_flexcan_privs[NUM_FLEXCAN_INTERFACES] = { NULL };
+struct secv_tx_ring_buffers *secv_flexcan_tx_rings[NUM_FLEXCAN_INTERFACES] = { NULL };
+struct secv_rx_ring_buffers *secv_flexcan_rx_rings[NUM_FLEXCAN_INTERFACES] = { NULL };
+static struct secv_tx_ring_buffers secv_local_flexcan_tx_rings[NUM_FLEXCAN_INTERFACES];
+static struct secv_rx_ring_buffers secv_local_flexcan_rx_rings[NUM_FLEXCAN_INTERFACES];
+static struct timer_list secv_tx_timers[NUM_FLEXCAN_INTERFACES];
+static spinlock_t secv_xmit_locks[NUM_FLEXCAN_INTERFACES];
+static spinlock_t secv_recv_locks[NUM_FLEXCAN_INTERFACES];
+
+static struct net_device* secv_flexcan_devs[NUM_FLEXCAN_INTERFACES] = {NULL};
+static u32 waiting_tx_complete[NUM_FLEXCAN_INTERFACES] = {0};
+static u32 tx_pending[NUM_FLEXCAN_INTERFACES] = {0};
+static struct tx_mb secv_leftover_tx_mbs[NUM_FLEXCAN_INTERFACES];
+
 #define DRV_NAME			"flexcan"
 
 /* 8 for RX fifo and 2 error handling */
@@ -530,12 +563,12 @@ static inline void flexcan_write_be(u32 val, void __iomem *addr)
 
 static inline u32 flexcan_read_le(void __iomem *addr)
 {
-	return ioread32(addr);
+	return secv_read_le(addr);
 }
 
 static inline void flexcan_write_le(u32 val, void __iomem *addr)
 {
-	iowrite32(val, addr);
+	secv_write_le(val, addr);
 }
 
 static u32 flexcan_get_timestamp(struct flexcan_priv *priv)
@@ -550,6 +583,145 @@ static u32 flexcan_get_timestamp(struct flexcan_priv *priv)
 	return timestamp << 16;
 }
 
+/* SECV: Memory barrier helpers for OP-TEE */
+static inline void secv_wmb(void)
+{
+    asm volatile ("dmb st" : : : "memory");
+}
+
+static inline void secv_rmb(void)
+{
+    asm volatile ("dmb ld" : : : "memory");
+}
+
+
+static inline void secv_dsb(void)
+{
+    asm volatile ("dsb sy" : : : "memory");
+}
+
+// SECV: handle an IPI/SGI from the SSMe
+static void secv_ipi_handler(void* dev_id) {
+	// Handle the SGI from SSMe. 
+	u32 interface_index;
+	struct flexcan_priv* nw_priv;
+	struct secv_flexcan_priv *priv;
+	unsigned long flags;
+	u32 tx_count;
+	u64 interrupts_bitmap;
+	struct secv_tx_ring_buffers *local_tx_ring;
+	struct net_device* dev;
+	struct tx_mb* tx_mb;
+	struct net_device_stats *stats; 
+
+	struct secv_rx_ring_buffers *local_rx_ring, *global_rx_ring;
+	struct rx_mb* rx_mb;
+	u32 rx_count;
+	u32 ret, recvd;
+	struct secv_can_buffer_rec *rec;
+	void* addr;
+	void* base;
+
+	//printk(KERN_ALERT "Handling SGI from SSMe\n");
+
+	for(interface_index=0; interface_index < NUM_FLEXCAN_INTERFACES; interface_index++) {
+		dev = secv_flexcan_devs[interface_index];
+		stats = &dev->stats;
+		nw_priv = netdev_priv(dev);
+		priv = secv_flexcan_privs[interface_index];
+		
+		secv_rmb();
+		interrupts_bitmap = priv->interrupts_bitmap;
+
+		if(interrupts_bitmap == 0) {
+			//printk(KERN_ALERT "Skipping interface %d as its bitmap is not set %lx\n", interface_index, (unsigned long)&priv->interrupts_bitmap);
+			continue;
+		}
+		
+		if((interrupts_bitmap & (1 << SECV_SEND_COMPLETE_BIT_IDX)) != 0) {
+
+			spin_lock_irqsave(&secv_xmit_locks[interface_index], flags);
+
+			local_tx_ring = &secv_local_flexcan_tx_rings[interface_index];
+
+			for(tx_count = 0; tx_count < local_tx_ring->count; tx_count++) {
+				tx_mb = &local_tx_ring->mbs[tx_count];
+				stats->tx_bytes += can_rx_offload_get_echo_skb_queue_timestamp(&nw_priv->offload, tx_count,
+								    tx_mb->can_ctrl_tx << 16, NULL);
+				stats->tx_packets++;
+				addr = phys_to_virt(tx_mb->data);
+				base = (void*)((size_t)addr & ~((size_t)0xFFF));
+				size_t bit = ((char*)addr - (char*)base - sizeof(struct secv_can_buffer_rec))/32;
+				long rec_flags;
+				rec = (struct secv_can_buffer_rec*)base;
+				spin_lock_irqsave(&rec->lock, rec_flags);
+				rec->available &= ~((size_t)1 << bit);
+				spin_unlock_irqrestore(&rec->lock, rec_flags);
+			}
+			
+			memset(&secv_local_flexcan_tx_rings[interface_index], 0, sizeof(struct secv_tx_ring_buffers));
+
+			if(tx_pending[interface_index] != 0) {
+				memcpy((void*)&local_tx_ring->mbs[0], &secv_leftover_tx_mbs[interface_index], sizeof(struct tx_mb));
+				local_tx_ring->count++;
+				tx_pending[interface_index] = 0;
+				memset(&secv_leftover_tx_mbs[interface_index], 0, sizeof(struct tx_mb));
+				secv_wmb();
+				mod_timer(&secv_tx_timers[interface_index], jiffies + msecs_to_jiffies(SECV_QUEUE_WAIT_MS));
+			}
+
+			netif_wake_queue(dev);
+
+			secv_rmb();
+			priv->interrupts_bitmap &= ~(1 << SECV_SEND_COMPLETE_BIT_IDX);
+			secv_wmb();
+
+			waiting_tx_complete[interface_index] = 0;
+
+			spin_unlock_irqrestore(&secv_xmit_locks[interface_index], flags);
+			
+		} else if((interrupts_bitmap & (1 << SECV_RECV_BIT_IDX)) != 0) {
+			spin_lock_irqsave(&secv_recv_locks[interface_index], flags);
+
+			//printk(KERN_ALERT "Working on Flexcan Rx\n");
+			local_rx_ring = &secv_local_flexcan_rx_rings[interface_index];
+			global_rx_ring = secv_flexcan_rx_rings[interface_index];
+
+			//printk(KERN_ALERT "Flexcan Rx on interface %d, %lx %lx %lx\n", interface_index, (unsigned long)global_rx_ring, (unsigned long)local_rx_ring, (unsigned long)global_rx_ring->count);
+			rx_count = global_rx_ring->count;
+			local_rx_ring->count = rx_count;
+			recvd = 0;
+			
+			memcpy((void*)&local_rx_ring->mbs[0], (void*)&global_rx_ring->mbs[0], rx_count*sizeof(struct rx_mb));
+			//printk(KERN_ALERT "Offloading Flexcan Rx");
+			for(int i=0; i<rx_count; i++) {
+				//printk(KERN_ALERT "Offloading a CAN message with ID: %u %u\n", global_rx_ring->mbs[i].can_id, local_rx_ring->mbs[i].can_ctrl);
+				ret = secv_can_rx_offload_irq_offload_timestamp(&nw_priv->offload, &local_rx_ring->mbs[i]);
+				if(!ret) {
+					//printk(KERN_ALERT "Nothing offloaded.\n");
+					break;
+				}
+				recvd = 1;
+			}
+
+			secv_rmb();
+			priv->interrupts_bitmap &= ~(1 << SECV_RECV_BIT_IDX);
+			secv_wmb();
+
+			spin_unlock_irqrestore(&secv_recv_locks[interface_index], flags);
+
+			if(recvd) {
+				can_rx_offload_irq_finish(&nw_priv->offload);
+			}
+
+		} else {
+			// something went wrong
+			panic("Wrong bits set for SECV Flexcan interrupt?");
+		}
+	}
+
+}
+
 static struct flexcan_mb __iomem *flexcan_get_mb(const struct flexcan_priv *priv,
 						 u8 mb_index)
 {
@@ -853,11 +1025,49 @@ static netdev_tx_t flexcan_start_xmit(struct sk_buff *skb, struct net_device *de
 	u32 data;
 	u32 ctrl = FLEXCAN_MB_CODE_TX_DATA | ((can_fd_len2dlc(cfd->len)) << 16);
 	int i;
+	unsigned long flags;
+
+	// SECV:
+	u32 interface_index;
+	struct secv_flexcan_priv *secv_priv;
+	struct secv_tx_ring_buffers *local_tx_ring, *global_tx_ring;
+	struct tx_mb* mb;
+	u32 tx_index;
+	u32 actual_send;
+
+	// SECV: Get the interface index
+	interface_index = get_interface_index(priv->regs);
+	//printk(KERN_ALERT "transmitting on interface: %d\n", interface_index);
+	if(interface_index != 0 && interface_index != 1) {
+		netdev_err(dev, "Invalid interface index: %u\n", interface_index);
+		return NETDEV_TX_OK;
+	}
+
+	//printk(KERN_ALERT "Scheduling for transmission\n");
+	spin_lock_irqsave(&secv_xmit_locks[interface_index], flags);
 
-	if (can_dev_dropped_skb(dev, skb))
+	if (can_dev_dropped_skb(dev, skb)){
+		//printk(KERN_ALERT "SKB dropped\n");
+		spin_unlock_irqrestore(&secv_xmit_locks[interface_index], flags);
 		return NETDEV_TX_OK;
+	}
 
-	netif_stop_queue(dev);
+	
+
+	// SECV: transmission, do we have space in the TX ring buffer?
+	if(!waiting_tx_complete[interface_index]){
+		local_tx_ring = &secv_local_flexcan_tx_rings[interface_index];
+		tx_index = local_tx_ring->count;
+		local_tx_ring->count++;
+
+		actual_send = 0;
+		if (local_tx_ring->count == NUM_TX_BUFFERS) {
+			actual_send = 1;
+			netif_stop_queue(dev);
+		}
+	} else {
+		netif_stop_queue(dev);
+	}
 
 	if (cfd->can_id & CAN_EFF_FLAG) {
 		can_id = cfd->can_id & CAN_EFF_MASK;
@@ -876,24 +1086,65 @@ static netdev_tx_t flexcan_start_xmit(struct sk_buff *skb, struct net_device *de
 			ctrl |= FLEXCAN_MB_CNT_BRS;
 	}
 
-	for (i = 0; i < cfd->len; i += sizeof(u32)) {
-		data = be32_to_cpup((__be32 *)&cfd->data[i]);
-		priv->write(data, &priv->tx_mb->data[i / sizeof(u32)]);
-	}
+	//memcpy(mb->data, cfd->data, sizeof(*cfd));
 
-	can_put_echo_skb(skb, dev, 0, 0);
+	// for (i = 0; i < cfd->len; i += sizeof(u32)) {
+	// 	data = be32_to_cpup((__be32 *)&cfd->data[i]);
+	// 	priv->write(data, &priv->tx_mb->data[i / sizeof(u32)]);
+	// }
 
-	priv->write(can_id, &priv->tx_mb->can_id);
-	priv->write(ctrl, &priv->tx_mb->can_ctrl);
+	// SECV: put this frame at tx_index, it will be dequeued
+	// when SSMe signals end of transmission.
+	if(!waiting_tx_complete[interface_index]) {
+		mb = &local_tx_ring->mbs[tx_index];
+		mb->can_id = can_id;
+		mb->can_ctrl = ctrl;
+		mb->data = (struct canfd_frame*)(*((u64*)&cfd->data[0]));
 
-	/* Errata ERR005829 step8:
-	 * Write twice INACTIVE(0x8) code to first MB.
-	 */
-	priv->write(FLEXCAN_MB_CODE_TX_INACTIVE,
-		    &priv->tx_mb_reserved->can_ctrl);
-	priv->write(FLEXCAN_MB_CODE_TX_INACTIVE,
-		    &priv->tx_mb_reserved->can_ctrl);
+		//printk(KERN_ALERT "Queueing CAN ID: %x CAN CTRL: %x data: %lx\n", mb->can_id, mb->can_ctrl, (unsigned long)mb->data);
+
+		can_put_echo_skb(skb, dev, tx_index, 0);
+	
+
+		// priv->write(can_id, &priv->tx_mb->can_id);
+		// priv->write(ctrl, &priv->tx_mb->can_ctrl);
+
+		/* Errata ERR005829 step8:
+		* Write twice INACTIVE(0x8) code to first MB.
+		*/
+		// priv->write(FLEXCAN_MB_CODE_TX_INACTIVE,
+		// 	    &priv->tx_mb_reserved->can_ctrl);
+		// priv->write(FLEXCAN_MB_CODE_TX_INACTIVE,
+		// 	    &priv->tx_mb_reserved->can_ctrl);
+		//printk(KERN_ALERT "Scheduling CAN message for sending\n");
+		if(actual_send) {
+			// SECV: trigger transmission, set tx_complete waiting flag
+			//printk(KERN_ALERT "Sending CAN %d messages\n", local_tx_ring->count);
+
+
+			global_tx_ring = secv_flexcan_tx_rings[interface_index];
+			global_tx_ring->count = local_tx_ring->count;
+
+			memcpy((void*)&global_tx_ring->mbs[0], (void*)&local_tx_ring->mbs[0], sizeof(struct tx_mb)*local_tx_ring->count);
+
+			waiting_tx_complete[interface_index] = 1;
+			del_timer(&secv_tx_timers[interface_index]);
 
+			secv_dsb();
+			secv_flexcan_trigger_tx(interface_index);
+		}else if(tx_index == 0) {
+			//printk(KERN_ALERT "Setting timer to wait for more messages\n");
+			mod_timer(&secv_tx_timers[interface_index], jiffies + msecs_to_jiffies(SECV_QUEUE_WAIT_MS));
+		}
+	} else {
+		mb = &secv_leftover_tx_mbs[interface_index];
+		mb->can_id = can_id;
+		mb->can_ctrl = ctrl;
+		mb->data = (struct canfd_frame*)(*((u64*)&cfd->data[0]));
+		tx_pending[interface_index] = 1;
+	}
+
+	spin_unlock_irqrestore(&secv_xmit_locks[interface_index], flags);
 	return NETDEV_TX_OK;
 }
 
@@ -1116,6 +1367,73 @@ static inline struct flexcan_priv *rx_offload_to_priv(struct can_rx_offload *off
 	return container_of(offload, struct flexcan_priv, offload);
 }
 
+/**
+ * SECV: offload an rx_mb received from SSMe.
+ */
+static struct sk_buff *secv_flexcan_mailbox_read(struct can_rx_offload *offload,
+						void* mb, u32* timestamp, bool drop)
+{
+	struct sk_buff* skb;
+	struct canfd_frame* cfd;
+	u32 reg_ctrl;
+	u32 reg_id, i;
+	struct rx_mb* rx_mb = (struct rx_mb*)mb;
+	struct flexcan_priv *priv = rx_offload_to_priv(offload);
+	u32 *data_ptr;
+
+	reg_ctrl = rx_mb->can_ctrl;
+
+	if (unlikely(drop)) {
+		skb = ERR_PTR(-ENOBUFS);
+		goto done;
+	}
+
+	//printk(KERN_ALERT "Reading the mailbox for real\n");
+	if (rx_mb->can_ctrl & FLEXCAN_MB_CNT_EDL)
+		skb = alloc_canfd_skb(offload->dev, &cfd);
+	else
+		skb = alloc_can_skb(offload->dev, (struct can_frame **)&cfd);
+	
+	if (unlikely(!skb)) {
+		//printk(KERN_ALERT "Couldnt allocate SKB\n");
+		skb = ERR_PTR(-ENOMEM);
+		goto done;
+	}
+
+	/* increase timstamp to full 32 bit */
+	*timestamp = reg_ctrl << 16;
+
+	reg_id = rx_mb->can_id;
+	if (reg_ctrl & FLEXCAN_MB_CNT_IDE)
+		cfd->can_id = ((reg_id >> 0) & CAN_EFF_MASK) | CAN_EFF_FLAG;
+	else
+		cfd->can_id = (reg_id >> 18) & CAN_SFF_MASK;
+
+	if (reg_ctrl & FLEXCAN_MB_CNT_EDL) {
+		cfd->len = can_fd_dlc2len((reg_ctrl >> 16) & 0xf);
+
+		if (reg_ctrl & FLEXCAN_MB_CNT_BRS)
+			cfd->flags |= CANFD_BRS;
+	} else {
+		cfd->len = can_cc_dlc2len((reg_ctrl >> 16) & 0xf);
+
+		if (reg_ctrl & FLEXCAN_MB_CNT_RTR)
+			cfd->can_id |= CAN_RTR_FLAG;
+	}
+
+	if (reg_ctrl & FLEXCAN_MB_CNT_ESI)
+		cfd->flags |= CANFD_ESI;
+
+	for (i = 0; i < cfd->len; i += sizeof(u32)) {
+		data_ptr = (u32*)(rx_mb->data.data + i);
+		__be32 data = cpu_to_be32(*data_ptr);
+		*(__be32 *)(cfd->data + i) = data;
+	}
+	//printk(KERN_ALERT "Finished offloading CAN ID: %x CAN CTRL: %x\n", cfd->can_id, reg_ctrl);
+done:
+	return skb;
+}
+
 static struct sk_buff *flexcan_mailbox_read(struct can_rx_offload *offload,
 					    unsigned int n, u32 *timestamp,
 					    bool drop)
@@ -1506,6 +1824,8 @@ static int flexcan_rx_offload_setup(struct net_device *dev)
 	priv->tx_mask = FLEXCAN_IFLAG_MB(priv->tx_mb_idx);
 
 	priv->offload.mailbox_read = flexcan_mailbox_read;
+	// SECV: set the read function.
+	priv->offload.secv_mailbox_read = secv_flexcan_mailbox_read;
 
 	if (priv->devtype_data.quirks & FLEXCAN_QUIRK_USE_RX_MAILBOX) {
 		priv->offload.mb_first = FLEXCAN_RX_MB_RX_MAILBOX_FIRST;
@@ -1837,6 +2157,11 @@ static int flexcan_open(struct net_device *dev)
 	int err;
 	u32 i, j, last, irq_array_len = ARRAY_SIZE(flexcan_irq_handlers);
 
+	// SECV: initialize nsec for secv_priv 
+	struct secv_flexcan_priv *secv_priv;
+	u32 interface_index;
+
+
 	if ((priv->can.ctrlmode & CAN_CTRLMODE_3_SAMPLES) &&
 	    (priv->can.ctrlmode & CAN_CTRLMODE_FD)) {
 		netdev_err(dev, "Three Samples mode and CAN-FD mode can't be used together\n");
@@ -1877,6 +2202,31 @@ static int flexcan_open(struct net_device *dev)
 				}
 			}
 
+	interface_index = get_interface_index(priv->regs);
+	if(interface_index == 0 || interface_index == 1) {
+		secv_priv = secv_flexcan_privs[interface_index];
+		secv_priv->regs = (struct flexcan_regs *)translate_el1(priv->regs);
+		secv_priv->tx_mb = (struct flexcan_mb *)translate_el1(priv->tx_mb);
+		secv_priv->tx_mb_reserved = (struct flexcan_mb *)translate_el1(priv->tx_mb_reserved);
+		secv_priv->interface_index = interface_index;
+		secv_priv->tx_mb_idx = priv->tx_mb_idx;
+		secv_priv->mb_count = priv->mb_count;
+		secv_priv->mb_size = priv->mb_size;
+		secv_priv->rx_mask = priv->rx_mask;
+		secv_priv->tx_mask = priv->tx_mask;
+		secv_priv->quirks = priv->devtype_data.quirks;
+		secv_priv->offload_mb_first = priv->offload.mb_first;
+		secv_priv->offload_mb_last = priv->offload.mb_last;
+		secv_priv->reg_ctrl_default = priv->reg_ctrl_default;
+		secv_priv->nsec_initialized = 1;
+		memset((void*)&secv_local_flexcan_tx_rings[interface_index], 0, sizeof(struct secv_tx_ring_buffers));
+		/** SECV: notify the secure world we have initialized this interface.
+		 *  ATF: trap this SMC call and disable NW access to this interface memory by reconfiguring XRDC for CAN<interface_idx> access control.
+		 *  OPTEE: copy the initialization info from the shared priv to the secure one. 
+		 *  From here on, the shared interface object is only used for sharing state info between NW & SW driver.*/ 
+		secv_flexcan_init_notify(interface_index);
+	}
+
 	flexcan_chip_interrupts_enable(dev);
 
 	netif_start_queue(dev);
@@ -2187,6 +2537,50 @@ static unsigned long get_per_clk_rate(struct clk *clk)
 	return rate;
 }
 
+static void secv_tx_timer_callback(struct timer_list *t) {
+	// TODO: send the frames we have now.
+	struct secv_tx_ring_buffers *local_tx_ring, *global_tx_ring;
+	u_int32_t count;
+	unsigned long flags;
+
+	u32 interface_index = t == &secv_tx_timers[0]? 0: t == &secv_tx_timers[1] ? 1: -1;
+
+	if(interface_index != 0 && interface_index != 1) {
+		// we have a problem
+		return;
+	}
+	
+	spin_lock_irqsave(&secv_xmit_locks[interface_index], flags);
+	if(!waiting_tx_complete[interface_index]) {
+
+		local_tx_ring = &secv_local_flexcan_tx_rings[interface_index];
+		global_tx_ring = secv_flexcan_tx_rings[interface_index];
+		count = local_tx_ring->count;
+		global_tx_ring->count = count;
+		
+		//printk(KERN_ALERT "SECV timer for transmission has expired with %d messages queued, interface%d\n", count, interface_index);
+
+		memcpy((void*)&global_tx_ring->mbs[0], (void*)&local_tx_ring->mbs[0], sizeof(struct tx_mb)*local_tx_ring->count);
+		//memcpy((void*)global_tx_ring, (void*)&local_tx_ring, sizeof(struct secv_tx_ring_buffers));
+		//secv_dsb();
+
+		// while(count > 0) {
+		// 	//global_tx_ring->mbs[count-1] = local_tx_ring->mbs[count-1];
+		// 	//printk(KERN_ALERT "Message: %x CAN ID %x, %x CAN ctrl %x, %x data ptr: %lx, %lx\n", count, 
+		// 		global_tx_ring->mbs[count-1].can_id, local_tx_ring->mbs[count-1].can_id,
+		// 		global_tx_ring->mbs[count-1].can_ctrl, local_tx_ring->mbs[count-1].can_ctrl,
+		// 		(unsigned long)global_tx_ring->mbs[count-1].data, (unsigned long)local_tx_ring->mbs[count-1].data );
+		// 	count--;
+		// }
+
+		waiting_tx_complete[interface_index] = 1;
+
+		secv_dsb();
+		secv_flexcan_trigger_tx(interface_index);
+	}
+	spin_unlock_irqrestore(&secv_xmit_locks[interface_index], flags);
+}
+
 static int flexcan_probe(struct platform_device *pdev)
 {
 	const struct of_device_id *of_id;
@@ -2203,6 +2597,56 @@ static int flexcan_probe(struct platform_device *pdev)
 	u32 i, clock_freq = 0;
 	bool named_irqs;
 
+	// SECV: declarations
+	struct device_node *rmem_node;
+	struct reserved_mem *rmem;
+	phys_addr_t secv_shmem_phys_addr;
+	size_t secv_shmem_size;
+	u32 interface_index;
+
+	// SECV shared memory mapping
+	if(secv_shmem_virt_addr == NULL) {
+		rmem_node = of_find_node_by_path("/reserved-memory/sorhta_shmem@E0000000");
+		if (!rmem_node) {
+			dev_err(&pdev->dev, "No secv-shmem node found in DT\n");
+			return -ENODEV;
+		}
+		
+
+		rmem = of_reserved_mem_lookup(rmem_node);
+		of_node_put(rmem_node);
+		if (!rmem) {
+			dev_err(&pdev->dev, "No reserved memory found for secv-shmem\n");
+			return -ENODEV;
+		}
+
+		secv_shmem_phys_addr = rmem->base;
+		secv_shmem_size = rmem->size;
+
+		secv_shmem_virt_addr = ioremap(secv_shmem_phys_addr, secv_shmem_size);
+		if (!secv_shmem_virt_addr) {
+			dev_err(&pdev->dev, "Failed to ioremap secv-shmem\n");
+			return -ENOMEM;
+		}
+
+		secv_flexcan_privs[0] = (struct secv_flexcan_priv *)((char*)secv_shmem_virt_addr + SORHTA_PRIV0_BASE_OFF);
+		secv_flexcan_privs[1] = (struct secv_flexcan_priv *)((char*)secv_shmem_virt_addr + SORHTA_PRIV1_BASE_OFF);
+
+		secv_flexcan_rx_rings[0] = (struct secv_rx_ring_buffers *)((char*)secv_shmem_virt_addr + SORHTA_FLEXCAN_RX_SHMEM_BASE0_OFF);
+		secv_flexcan_rx_rings[1] = (struct secv_rx_ring_buffers *)((char*)secv_shmem_virt_addr + SORHTA_FLEXCAN_RX_SHMEM_BASE1_OFF);
+
+		secv_flexcan_tx_rings[0] = (struct secv_tx_ring_buffers *)((char*)secv_shmem_virt_addr + SORHTA_FLEXCAN_TX_SHMEM_BASE0_OFF);
+		secv_flexcan_tx_rings[1] = (struct secv_tx_ring_buffers *)((char*)secv_shmem_virt_addr + SORHTA_FLEXCAN_TX_SHMEM_BASE1_OFF);
+
+		timer_setup(&secv_tx_timers[0], secv_tx_timer_callback, 0);
+		timer_setup(&secv_tx_timers[1], secv_tx_timer_callback, 0);
+
+		arch_set_secv_ipi_handler(secv_ipi_handler);
+
+		dev_info(&pdev->dev, "SECV shared memory mapped at phys: 0x%pa, virt: %p, size: %zu\n",
+			 &secv_shmem_phys_addr, secv_shmem_virt_addr, secv_shmem_size);
+	}
+
 	reg_xceiver = devm_regulator_get_optional(&pdev->dev, "xceiver");
 	if (PTR_ERR(reg_xceiver) == -EPROBE_DEFER)
 		return -EPROBE_DEFER;
@@ -2316,7 +2760,9 @@ static int flexcan_probe(struct platform_device *pdev)
 		return -EINVAL;
 	}
 
-	dev = alloc_candev(sizeof(struct flexcan_priv), 1);
+	// SECV: we modify this to allow the echo queue to wait for multiple 
+	// echo queues from the SSMe.
+	dev = alloc_candev(sizeof(struct flexcan_priv), NUM_TX_BUFFERS);
 	if (!dev)
 		return -ENOMEM;
 
@@ -2399,6 +2845,14 @@ static int flexcan_probe(struct platform_device *pdev)
 		dev_info(&pdev->dev, " %d", irq_nos[i]);
 	dev_info(&pdev->dev, ")\n");
 
+	// SECV: initialize the tx locks and save the dev.
+	// too lazy to find out a better way around this.
+	interface_index = get_interface_index((void*)regs);
+	// TODO: check that interface index is okay. we could have more than 2 interfaces in reality
+	spin_lock_init(&secv_xmit_locks[interface_index]);
+	spin_lock_init(&secv_recv_locks[interface_index]);
+	secv_flexcan_devs[interface_index] = dev;
+
 	return 0;
 
  failed_setup_stop_mode:
diff --git a/drivers/net/can/flexcan/secv_flexcan.c b/drivers/net/can/flexcan/secv_flexcan.c
new file mode 100644
index 000000000000..bc8d23286afd
--- /dev/null
+++ b/drivers/net/can/flexcan/secv_flexcan.c
@@ -0,0 +1,5 @@
+#include "secv_flexcan.h"
+
+
+
+
diff --git a/drivers/net/can/flexcan/secv_flexcan.h b/drivers/net/can/flexcan/secv_flexcan.h
new file mode 100644
index 000000000000..67b8ed729645
--- /dev/null
+++ b/drivers/net/can/flexcan/secv_flexcan.h
@@ -0,0 +1,97 @@
+#ifndef SECV_FLEXCAN_H
+#define SECV_FLEXCAN_H
+
+#include "secv_tee.h"
+#include "flexcan.h"
+
+#define NUM_FLEXCAN_INTERFACES      2
+#define RX_RING_SIZE                512
+#define FRAME_COUNT_PER_MB          16
+#define NUM_TX_BUFFERS              1 // This greatly affects the throughput based on size. Large size (32, 64 bytes) messages benefit from large queue, while small size (8, 16 bytes) suffer with message loss due to it. 
+#define NUM_RX_BUFFERS              FRAME_COUNT_PER_MB * NUM_TX_BUFFERS
+#define MAX_MBS                     128
+
+#define S32_FLEXCAN0_BASE_ADDR          0x401B4000UL
+#define S32_FLEXCAN1_BASE_ADDR          0x401BE000UL
+#define S32_FLEXCAN_SIZE                0xA000UL
+
+#define SECV_SEND_COMPLETE_BIT_IDX		0x0
+#define SECV_RECV_BIT_IDX				0x1
+#define SECV_QUEUE_WAIT_MS              0x1
+
+/**
+ * SECV: this is the reduced state, for device control. shared between the inner kernel & SW driverlet.
+ * For security, this must be stored in the inner kernel region at the NW end.
+ * OK to be read by outer kernel, risky to be written by outer kernel as a compromised 
+ * outer kernel can use it to control the SW driverlet instead.
+ */
+struct secv_flexcan_priv {
+    struct flexcan_regs *regs;
+    struct flexcan_mb   *tx_mb;
+    struct flexcan_mb   *tx_mb_reserved;
+    uint8_t interface_index;
+    uint8_t nsec_initialized;
+    uint8_t sec_initialized;
+    uint8_t tx_mb_idx;
+    uint8_t mb_count;
+    uint8_t mb_size;
+    uint64_t rx_mask;
+    uint64_t tx_mask;
+    uint32_t reg_ctrl_default;
+    uint32_t quirks;
+    uint32_t offload_mb_first;
+    uint32_t offload_mb_last;
+    u_int64_t interrupts_bitmap;
+};
+
+
+struct rx_mb {
+    uint32_t can_ctrl;
+    uint32_t can_id;
+    struct canfd_frame data;
+};
+
+struct tx_mb {
+    uint32_t can_ctrl;
+    uint32_t can_ctrl_tx;
+    uint32_t can_id;
+    struct canfd_frame *data;
+};
+
+struct secv_tx_ring_buffers {
+    struct tx_mb mbs[NUM_TX_BUFFERS];
+    size_t count;
+};
+
+
+struct secv_rx_ring_buffers {
+    struct rx_mb mbs[NUM_RX_BUFFERS];
+    size_t count;
+};
+
+#define SORHTA_PRIV0_BASE_OFF           (PAGE_SIZE)
+#define SORHTA_PRIV1_BASE_OFF           (PAGE_SIZE * 2)
+#define SORHTA_FLEXCAN_RX_SHMEM_BASE0_OFF	(PAGE_SIZE * 3)
+#define SORHTA_FLEXCAN_RX_SHMEM_SIZE0	    ((sizeof(struct secv_rx_ring_buffers) + PAGE_SIZE-1) & ~((size_t)0xFFF))
+#define SORHTA_FLEXCAN_RX_SHMEM_BASE1_OFF	(SORHTA_FLEXCAN_RX_SHMEM_BASE0_OFF + SORHTA_FLEXCAN_RX_SHMEM_SIZE0)
+#define SORHTA_FLEXCAN_RX_SHMEM_SIZE1	    (SORHTA_FLEXCAN_RX_SHMEM_SIZE0)
+
+#define SORHTA_FLEXCAN_TX_SHMEM_BASE0_OFF	(SORHTA_FLEXCAN_RX_SHMEM_BASE1_OFF + SORHTA_FLEXCAN_RX_SHMEM_SIZE1)
+#define SORHTA_FLEXCAN_TX_SHMEM_SIZE0	    ((sizeof(struct secv_tx_ring_buffers) + PAGE_SIZE-1) & ~((size_t)0xFFF))
+#define SORHTA_FLEXCAN_TX_SHMEM_BASE1_OFF	(SORHTA_FLEXCAN_TX_SHMEM_BASE0_OFF + SORHTA_FLEXCAN_TX_SHMEM_SIZE0)
+#define SORHTA_FLEXCAN_TX_SHMEM_SIZE1	    (SORHTA_FLEXCAN_TX_SHMEM_SIZE0)
+
+static inline u32 get_interface_index(void __iomem *regs)
+{
+    char* p = (char*)translate_el1((u64)regs);
+    if(p < (char*)S32_FLEXCAN1_BASE_ADDR && p >= (char*)S32_FLEXCAN0_BASE_ADDR  ) {
+        return 0;
+    } else if(p >= (char*)S32_FLEXCAN1_BASE_ADDR && p < (char*)(S32_FLEXCAN1_BASE_ADDR + S32_FLEXCAN_SIZE) ) {
+        return 1;
+    } 
+
+    return 0xFFFFFFFF;
+}
+
+
+#endif
\ No newline at end of file
diff --git a/drivers/net/can/flexcan/secv_tee.h b/drivers/net/can/flexcan/secv_tee.h
new file mode 100644
index 000000000000..c217a100792b
--- /dev/null
+++ b/drivers/net/can/flexcan/secv_tee.h
@@ -0,0 +1,148 @@
+#ifndef SECV_TEE_H
+#define SECV_TEE_H
+
+#include <linux/arm-smccc.h>
+#include <linux/atomic.h>
+#include <linux/string.h>
+#include <linux/types.h>
+
+#define MASK_47_TO_12   0x0000FFFFFFFFF000ULL
+#define MASK_11_TO_0    0x000000000000FFFULL
+
+#define SECV_SHMEM_BASE 0xE2000000ULL
+#define SECV_SHMEM_SIZE 0x2000ULL
+
+/* Map a NW virtual address to a physical address */
+__attribute__((always_inline)) static inline u64 translate_el1(u64 virt_addr)
+{
+    u64 phys_addr = virt_addr;
+    asm("AT S1E1R, %[a];"
+        "MRS %[a], PAR_EL1;"
+        : [a] "+r"(phys_addr));
+
+    if((1 & phys_addr)) {
+        //printk(KERN_ALERT "EL1 translation fault %llx for addr 0xllx\n", phys_addr, virt_addr);
+        return phys_addr;
+    }
+    phys_addr = (MASK_47_TO_12 & phys_addr) + (MASK_11_TO_0 & virt_addr);
+    return phys_addr;
+}
+
+
+#define OPTEE_SMC_STD_CALL_VAL(func_num) \
+            ARM_SMCCC_CALL_VAL(ARM_SMCCC_STD_CALL, ARM_SMCCC_SMC_32, \
+                ARM_SMCCC_OWNER_TRUSTED_OS, (func_num))
+
+#define OPTEE_SMC_FAST_CALL_VAL(func_num) \
+            ARM_SMCCC_CALL_VAL(ARM_SMCCC_FAST_CALL, ARM_SMCCC_SMC_32, \
+                ARM_SMCCC_OWNER_TRUSTED_OS, (func_num))
+
+
+#define OPTEE_FUNCID_CAN_INIT_NOTIFY        0xF1
+#define OPTEE_FUNCID_CAN_READ_BE            0xF2
+#define OPTEE_FUNCID_CAN_WRITE_BE           0xF3
+#define OPTEE_FUNCID_CAN_READ_LE            0xF4
+#define OPTEE_FUNCID_CAN_WRITE_LE           0xF5
+#define OPTEE_FUNCID_CAN_TRIGGER_TX         0xF6
+#define OPTEE_FUNCID_CAN_ALLOCATE_BUFFER    0xFC
+#define OPTEE_FUNCID_CAN_FREE_BUFFER        0xFD
+
+#define OPTEE_SMC_FUNCID_CAN_INIT_NOTIFY OPTEE_FUNCID_CAN_INIT_NOTIFY
+#define OPTEE_SMC_CALL_CAN_INIT_NOTIFY   \
+                OPTEE_SMC_FAST_CALL_VAL(OPTEE_SMC_FUNCID_CAN_INIT_NOTIFY)
+
+#define OPTEE_SMC_FUNCID_CAN_READ_BE    OPTEE_FUNCID_CAN_READ_BE
+#define OPTEE_SMC_CALL_CAN_READ_BE  \
+                OPTEE_SMC_FAST_CALL_VAL(OPTEE_SMC_FUNCID_CAN_READ_BE)
+
+#define OPTEE_SMC_FUNCID_CAN_WRITE_BE   OPTEE_FUNCID_CAN_WRITE_BE
+#define OPTEE_SMC_CALL_CAN_WRITE_BE \
+                OPTEE_SMC_FAST_CALL_VAL(OPTEE_SMC_FUNCID_CAN_WRITE_BE)
+
+#define OPTEE_SMC_FUNCID_CAN_READ_LE    OPTEE_FUNCID_CAN_READ_LE
+#define OPTEE_SMC_CALL_CAN_READ_LE  \
+                OPTEE_SMC_FAST_CALL_VAL(OPTEE_SMC_FUNCID_CAN_READ_LE)
+
+#define OPTEE_SMC_FUNCID_CAN_WRITE_LE    OPTEE_FUNCID_CAN_WRITE_LE
+#define OPTEE_SMC_CALL_CAN_WRITE_LE  \
+                OPTEE_SMC_FAST_CALL_VAL(OPTEE_SMC_FUNCID_CAN_WRITE_LE)
+
+#define OPTEE_SMC_FUNCID_CAN_TRIGGER_TX    OPTEE_FUNCID_CAN_TRIGGER_TX
+#define OPTEE_SMC_CALL_CAN_TRIGGER_TX  \
+                OPTEE_SMC_FAST_CALL_VAL(OPTEE_SMC_FUNCID_CAN_TRIGGER_TX)
+
+__attribute__((always_inline)) static inline uint32_t 
+secv_read_le(void* __iomem addr)
+{
+    struct arm_smccc_res res;
+    u64 address;
+    address = translate_el1((u64)addr);
+
+    arm_smccc_smc(OPTEE_SMC_CALL_CAN_READ_LE, // a0
+        address, // a1
+        0,  // a2
+        0,  // a3
+        0,  // a4
+        0,  // a5
+        0,  // a6
+        0,  // a7
+        &res
+    );
+    return (uint32_t)res.a1;
+}
+
+
+__attribute__((always_inline)) static inline void 
+secv_write_le(uint32_t val, void* __iomem addr)
+{
+    struct arm_smccc_res res;
+    u64 address;
+    address = translate_el1((u64)addr);
+
+    arm_smccc_smc(OPTEE_SMC_CALL_CAN_WRITE_LE, // a0
+        address, // a1
+        val,  // a2
+        0,  // a3
+        0,  // a4
+        0,  // a5
+        0,  // a6
+        0,  // a7
+        &res
+    );
+}
+
+__attribute__((always_inline)) static inline void
+secv_flexcan_trigger_tx(uint32_t interface_index)
+{
+    struct arm_smccc_res res;
+
+    arm_smccc_smc(OPTEE_SMC_CALL_CAN_TRIGGER_TX, // a0
+        interface_index, // a1
+        0,  // a2
+        0,  // a3
+        0,  // a4
+        0,  // a5
+        0,  // a6
+        0,  // a7
+        &res
+    );
+}
+
+__attribute__((always_inline)) static inline void
+secv_flexcan_init_notify(uint32_t interface_index)
+{
+    struct arm_smccc_res res;
+
+    arm_smccc_smc(OPTEE_SMC_CALL_CAN_INIT_NOTIFY, // a0
+        interface_index, // a1
+        0,  // a2
+        0,  // a3
+        0,  // a4
+        0,  // a5
+        0,  // a6
+        0,  // a7
+        &res
+    );
+}
+
+#endif
\ No newline at end of file
diff --git a/include/linux/can/rx-offload.h b/include/linux/can/rx-offload.h
index d29bb4521947..f1ffbd92d7e5 100644
--- a/include/linux/can/rx-offload.h
+++ b/include/linux/can/rx-offload.h
@@ -18,6 +18,9 @@ struct can_rx_offload {
 	struct sk_buff *(*mailbox_read)(struct can_rx_offload *offload,
 					unsigned int mb, u32 *timestamp,
 					bool drop);
+	
+	/** SECV: read from SSMe rx_mb instead of the peripheral memory */
+	struct sk_buff *(*secv_mailbox_read)(struct can_rx_offload *offload, void* mb, u32* timestamp, bool drop);
 
 	struct sk_buff_head skb_queue;
 	struct sk_buff_head skb_irq_queue;
@@ -41,6 +44,9 @@ int can_rx_offload_add_manual(struct net_device *dev,
 			      unsigned int weight);
 int can_rx_offload_irq_offload_timestamp(struct can_rx_offload *offload,
 					 u64 reg);
+// SECV
+int secv_can_rx_offload_irq_offload_timestamp(struct can_rx_offload *offload, void* rx_mb);
+
 int can_rx_offload_irq_offload_fifo(struct can_rx_offload *offload);
 int can_rx_offload_queue_timestamp(struct can_rx_offload *offload,
 				   struct sk_buff *skb, u32 timestamp);
diff --git a/include/linux/sched.h b/include/linux/sched.h
index c02fd12b49dc..919358b33d3c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1554,6 +1554,10 @@ struct task_struct {
 	 *
 	 * Do not put anything below here!
 	 */
+	/**
+	 * SECV: can_buffer_rec, TODO: move this to some special page in the inner kernel
+	 */
+	void *can_buffer_rec;
 };
 
 static inline struct pid *task_pid(struct task_struct *task)
diff --git a/net/can/af_can.c b/net/can/af_can.c
index 7343fd487dbe..9e5ca91d4050 100644
--- a/net/can/af_can.c
+++ b/net/can/af_can.c
@@ -64,6 +64,7 @@
 #include <net/sock.h>
 
 #include "af_can.h"
+#include <linux/vmalloc.h>
 
 MODULE_DESCRIPTION("Controller Area Network PF_CAN core");
 MODULE_LICENSE("Dual BSD/GPL");
@@ -211,6 +212,7 @@ int can_send(struct sk_buff *skb, int loop)
 
 		skb->protocol = htons(ETH_P_CANFD);
 
+		/* SECV: this breaks write permissions, can we avoid this by setting the bit earlier on? */
 		/* set CAN FD flag for CAN FD frames by default */
 		cfd->flags |= CANFD_FDF;
 	} else {
@@ -781,6 +783,231 @@ void can_proto_unregister(const struct can_proto *cp)
 }
 EXPORT_SYMBOL(can_proto_unregister);
 
+/**
+ * SECV: custom sk_buff free function, this will free our CAN sk_buff, and return the secure buffer.
+ */
+void secv_can_skb_destructor(struct sk_buff *skb) {
+	secv_task_free_frame_buffer(skb->data);
+	sock_wfree(skb);
+}
+
+static int secv_current_has_frame_buffer(void)
+{
+	struct task_struct *task = current;
+	return task->can_buffer_rec != NULL;
+}
+
+/**
+ * SECV: helper function to return the unset bits in value
+ */
+static inline void get_unset_bits(u_int64_t value, u_int8_t bits[64], size_t *count) {
+    *count = 0;
+    u_int64_t inverted = ~value;  // flip all bits
+    
+    while (inverted != 0) {
+        bits[*count] = __builtin_ctzll(inverted);
+        (*count)++;
+        inverted &= inverted - 1;
+    }
+}
+
+/**
+ * SECV: allocate a buffer record for the current task
+ */
+int secv_alloc_can_buffer_rec(struct task_struct *task)
+{
+	unsigned long phys_addr;
+	unsigned long user_virt_addr;
+	void *kernel_virt_addr;
+	struct vm_area_struct *vma;
+	struct mm_struct *mm = task->mm;
+	unsigned long pfn;
+	struct page *page;
+	int ret;
+	struct secv_can_buffer_rec rec;
+
+	if (!mm || task->can_buffer_rec)
+		return -EBUSY;
+	
+	// rec = kzalloc(sizeof(struct secv_can_buffer_rec), GFP_KERNEL);
+	// if (!rec)
+	// 	return -ENOMEM;
+
+	rec.available = 0;
+	rec.buffer = NULL;
+
+	// Request physical memory from SSMe
+	phys_addr = (unsigned long)secv_tee_allocate_can_buffer(task->pid);
+	if (!phys_addr) {
+		ret = -ENOMEM;
+		//printk(KERN_ALERT "SECV: Failed to allocate secure buffer\n");
+		goto free_rec;
+	}
+
+	//printk(KERN_ALERT "Allocated phys_addr: %lx\n", (unsigned long)phys_addr);
+
+	// Convert physical address to page
+	pfn = phys_addr >> PAGE_SHIFT;
+	
+	// Validate PFN before using
+	if (!pfn_valid(pfn)) {
+		ret = -EINVAL;
+		//printk(KERN_ALERT "SECV: Failed to get pfn for physical page: %lx\n", (unsigned long)phys_addr);
+		goto free_rec_buffer;
+	}
+	
+	page = pfn_to_page(pfn);
+	
+	// Increment page reference count
+	get_page(page);
+
+	// Create kernel mapping (read-only)
+	// SECV: TODO, move this to the inner kernel.
+	kernel_virt_addr = vmap(&page, 1, VM_MAP, PAGE_KERNEL);
+	if (!kernel_virt_addr) {
+		ret = -ENOMEM;
+		//printk(KERN_ALERT "SECV: Mapping secure buffer to kernel failed\n");
+		goto put_page;
+	}
+
+	rec.kernel_addr = kernel_virt_addr;  // Store kernel mapping
+	rec.phys_addr = phys_addr;
+	rec.next_bit = 0;
+	spin_lock_init(&rec.lock);
+
+	// Find available virtual address in process's address space
+	down_write(&mm->mmap_lock);
+	user_virt_addr = get_unmapped_area(NULL, 0, SECV_CAN_BUFFER_SIZE, 0, 0);
+	if (IS_ERR_VALUE(user_virt_addr)) {
+		ret = user_virt_addr;
+		//printk(KERN_ALERT "SECV: No umapped area for process\n");
+		goto unmap_kernel;
+	}
+
+	rec.buffer = (void *)user_virt_addr;
+
+	// Create VMA for this buffer
+	vma = vm_area_alloc(mm);
+	if (!vma) {
+		ret = -ENOMEM;
+		//printk(KERN_ALERT "SECV: Failed to allocated vmarea\n");
+		goto unmap_kernel;
+	}
+
+	vma->vm_start = user_virt_addr;
+	vma->vm_end = user_virt_addr + SECV_CAN_BUFFER_SIZE;
+	// Use vm_flags_init() instead of direct assignment
+	vm_flags_init(vma, VM_READ | VM_WRITE | VM_SHARED | VM_DONTEXPAND | VM_DONTDUMP);
+	
+	vma->vm_page_prot = pgprot_writecombine(vm_get_page_prot(vma->vm_flags));
+	vma->vm_pgoff = pfn;
+
+	// Map physical pages into user space with RW permissions
+	ret = remap_pfn_range(vma, user_virt_addr, pfn, SECV_CAN_BUFFER_SIZE, vma->vm_page_prot);
+	if (ret) {
+		//printk(KERN_ALERT "SECV: remapping pfn range failed\n");
+		goto free_vma;
+	}
+
+	// Insert VMA into process's address space
+	ret = insert_vm_struct(mm, vma);
+	if (ret) {
+		//printk(KERN_ALERT "SECV: inserting vm struct failed\n");
+		goto free_vma;
+	}
+	
+	up_write(&mm->mmap_lock);
+	// SECV TODO: use RW in inner kernel & RO in outer kernel.
+	task->can_buffer_rec = (void *)kernel_virt_addr;
+	memcpy((void*)kernel_virt_addr, (void*)&rec, sizeof(struct secv_can_buffer_rec));
+	// copy_to_user((void*)user_virt_addr, (void*)&rec, sizeof(struct secv_can_buffer_rec));
+	// secv_wmb();
+	// flush_dcache_page(page);
+	return 0;
+
+free_vma:
+	vm_area_free(vma);
+unmap_kernel:
+	up_write(&mm->mmap_lock);
+	vunmap(kernel_virt_addr);  // Use vunmap, not iounmap
+put_page:
+	put_page(page);  // Decrement page refcount
+free_rec_buffer:
+	secv_tee_free_can_buffer(task->pid);
+free_rec:
+	return ret;  // Return proper error code
+}
+
+
+/**
+ * SECV: free a secure can frame
+ */
+void secv_task_free_frame_buffer(void* frame)
+{
+	struct task_struct *task;
+	struct secv_can_buffer_rec* rec;
+	u32 bit_index;
+	size_t diff;
+
+	task = current;
+	rec = (struct secv_can_buffer_rec*)task->can_buffer_rec;
+	diff = (char*)frame - (char*)rec->buffer;
+	diff /= sizeof(struct canfd_frame);
+	rec->available &= ~(0x1 << diff);
+}
+
+/**
+ * SECV: allocate a secure frame buffer for the current struct
+ */
+struct canfd_frame* secv_task_alloc_frame_buffer(u_int64_t *secure_phys_addr) 
+{
+	struct task_struct *task;
+	size_t bit_count;
+	struct secv_can_buffer_rec* rec;
+	struct canfd_frame* frame;
+	u_int8_t bits[64] = {0};
+	u_int64_t available;
+	long flags;
+
+	task = current;
+	frame = NULL;
+	if(!secv_current_has_frame_buffer() && secv_alloc_can_buffer_rec(task))
+	{
+		return NULL;
+	}
+
+	rec = (struct secv_can_buffer_rec*)task->can_buffer_rec;
+	spin_lock_irqsave(&rec->lock, flags);
+	get_unset_bits(rec->available, bits, &bit_count);
+	if(rec->next_bit >= SECV_MAX_SHADOW_FRAMES || rec->available & (1 << rec->next_bit)) {
+		//printk(KERN_ALERT "Set bits: %d, Available: %x\n", bit_count, rec->available);
+		spin_unlock_irqrestore(&rec->lock, flags);
+		return NULL;
+	}
+	// get_unset_bits(rec->available, bits, &bit_count);
+	// if(bit_count <= (sizeof(rec->available)*8 - SECV_MAX_SHADOW_FRAMES))
+	// {
+	// 	printk(KERN_ALERT "Set bits: %d, Available: %x\n", bit_count, rec->available);
+	// 	return NULL;
+	// }
+	bits[0] = rec->next_bit;
+	rec->next_bit = (rec->next_bit + 1) % SECV_MAX_SHADOW_FRAMES;
+	//available = rec->available;
+	available |= (1 << bits[0]);
+	
+	//copy_to_user((void*)&rec->available, (void*)&available, sizeof(u_int64_t));
+	// SECV TODO use user address or do this inner kernel.
+	frame = (struct canfd_frame*)((char*)rec->kernel_addr + sizeof(struct canfd_frame)*bits[0] + ((sizeof(struct secv_can_buffer_rec)+0x7) & ~((size_t)0x7)));
+	rec->available = available;
+	//copy_to_user((void*)frame, (void*)&secv_empty_frame, sizeof(struct canfd_frame));
+	//printk(KERN_ALERT "Allocated frame: %lx %lx %lx\n", (unsigned long)rec, (unsigned long)frame, (unsigned long)rec->phys_addr);
+	*secure_phys_addr = rec->phys_addr + sizeof(struct canfd_frame)*bits[0] +  ((sizeof(struct secv_can_buffer_rec)+0x7) & ~((size_t)0x7));
+	// secv_wmb();
+	// flush_dcache_page((struct page*)rec->page);
+	spin_unlock_irqrestore(&rec->lock, flags);
+	return frame;
+}
+
 static int can_pernet_init(struct net *net)
 {
 	spin_lock_init(&net->can.rcvlists_lock);
diff --git a/net/can/af_can.h b/net/can/af_can.h
index 7c2d9161e224..3f8eeb9214b8 100644
--- a/net/can/af_can.h
+++ b/net/can/af_can.h
@@ -45,6 +45,85 @@
 #include <linux/list.h>
 #include <linux/rcupdate.h>
 #include <linux/can.h>
+#include <linux/spinlock_types.h>
+
+// SECV: related to secure frames
+#include <linux/sched.h>
+#include <linux/arm-smccc.h>
+
+#define SECV_MAX_SHADOW_FRAMES	32
+#define SECV_CAN_BUFFER_SIZE	PAGE_SIZE
+struct secv_can_buffer_rec {
+	void* buffer; // the pointer to the buffer base.
+	void* kernel_addr; // the kernel address base of this page.
+	u_int64_t phys_addr;
+	u_int64_t next_bit;
+	u_int64_t available; // tacks used frames
+	spinlock_t lock;
+};
+
+static struct canfd_frame secv_empty_frame = {.can_id = 0, .data = {0}, .flags = 0, .len = 0, .__res0 = 0, .__res1=0};
+
+#define OPTEE_FUNCID_CAN_ALLOCATE_BUFFER	0xFC
+#define OPTEE_FUNCID_CAN_FREE_BUFFER		0xFD
+
+#define OPTEE_SMC_FAST_CALL_VAL(func_num) \
+            ARM_SMCCC_CALL_VAL(ARM_SMCCC_FAST_CALL, ARM_SMCCC_SMC_32, \
+                ARM_SMCCC_OWNER_TRUSTED_OS, (func_num))
+
+#define OPTEE_SMC_FUNCID_CAN_ALLOCATE_BUFFER    OPTEE_FUNCID_CAN_ALLOCATE_BUFFER
+#define OPTEE_SMC_CALL_CAN_ALLOCATE_BUFFER  \
+                OPTEE_SMC_FAST_CALL_VAL(OPTEE_SMC_FUNCID_CAN_ALLOCATE_BUFFER)
+
+#define OPTEE_SMC_FUNCID_CAN_FREE_BUFFER    OPTEE_FUNCID_CAN_FREE_BUFFER
+#define OPTEE_SMC_CALL_CAN_FREE_BUFFER  \
+                OPTEE_SMC_FAST_CALL_VAL(OPTEE_SMC_FUNCID_CAN_FREE_BUFFER)
+
+__attribute__((always_inline)) static inline void __iomem*
+secv_tee_allocate_can_buffer(pid_t pid)
+{
+	struct arm_smccc_res res;
+	u64 pidu64 = (u64)pid;
+	arm_smccc_smc(OPTEE_SMC_CALL_CAN_ALLOCATE_BUFFER, // a0
+		pidu64, // a1
+		0, // a2
+		0, // a3
+		0, // a4
+		0, // a5
+		0, // a6
+		0, // a7
+		&res // res
+	);
+	return (void __iomem*)res.a1;
+}
+
+__attribute__((always_inline)) static inline void 
+secv_tee_free_can_buffer(pid_t pid)
+{
+	struct arm_smccc_res res;
+	u64 pidu64 = (u64)pid;
+	arm_smccc_smc(OPTEE_SMC_CALL_CAN_FREE_BUFFER, // a0
+		pidu64, // a1
+		0, // a2
+		0, // a3
+		0, // a4
+		0, // a5
+		0, // a6
+		0, // a7
+		&res // res
+	);
+}
+
+/** SEV: memory ordering functions */
+static inline void secv_wmb(void)
+{
+    asm volatile ("dmb st" : : : "memory");
+}
+
+static inline void secv_rmb(void)
+{
+    asm volatile ("dmb ld" : : : "memory");
+}
 
 /* af_can rx dispatcher structures */
 
@@ -100,4 +179,11 @@ void can_init_proc(struct net *net);
 void can_remove_proc(struct net *net);
 void can_stat_update(struct timer_list *t);
 
+
+// SECV:
+int secv_alloc_can_buffer_rec(struct task_struct *task);
+void secv_free_can_buffer_rec(struct task_struct *task);
+struct canfd_frame* secv_task_alloc_frame_buffer(u_int64_t*);
+void secv_task_free_frame_buffer(void* frame);
+struct sk_buff* secv_sock_alloc_send_skb(struct sock *sk, int noblock, int *errcode);
 #endif /* AF_CAN_H */
diff --git a/net/can/raw.c b/net/can/raw.c
index d50c3f3d892f..32590d1562db 100644
--- a/net/can/raw.c
+++ b/net/can/raw.c
@@ -56,6 +56,9 @@
 #include <net/sock.h>
 #include <net/net_namespace.h>
 
+// SECV: declarations.
+#include "af_can.h"
+
 MODULE_DESCRIPTION("PF_CAN raw protocol");
 MODULE_LICENSE("Dual BSD/GPL");
 MODULE_AUTHOR("Urs Thuermann <urs.thuermann@volkswagen.de>");
@@ -830,6 +833,9 @@ static int raw_sendmsg(struct socket *sock, struct msghdr *msg, size_t size)
 	struct sockcm_cookie sockc;
 	struct sk_buff *skb;
 	struct net_device *dev;
+	struct canfd_frame* secure_frame;
+	struct canfd_frame* data;
+	u_int64_t secure_phys_addr;
 	int ifindex;
 	int err = -EINVAL;
 
@@ -855,11 +861,20 @@ static int raw_sendmsg(struct socket *sock, struct msghdr *msg, size_t size)
 	if (!dev)
 		return -ENXIO;
 
+	// SECV: instead of allocating the CAN buffer from the outer kernel space,
+	secure_frame = secv_task_alloc_frame_buffer(&secure_phys_addr);
+	if(secure_frame == NULL) {
+		//printk(KERN_ALERT "Failed to allocate frame: 0x%lx 0x%lx\n", (unsigned long)secure_frame, (unsigned long)secure_phys_addr);
+		return -ENOMEM;
+	}
+
 	skb = sock_alloc_send_skb(sk, size + sizeof(struct can_skb_priv),
-				  msg->msg_flags & MSG_DONTWAIT, &err);
+	 			  msg->msg_flags & MSG_DONTWAIT, &err);
 	if (!skb)
 		goto put_dev;
 
+	//printk(KERN_ALERT "Allocated skb and built it: 0x%lx 0x%lx\n", (unsigned long)secure_frame, (unsigned long)secure_phys_addr);
+
 	can_skb_reserve(skb);
 	can_skb_prv(skb)->ifindex = dev->ifindex;
 	can_skb_prv(skb)->skbcnt = 0;
@@ -869,6 +884,17 @@ static int raw_sendmsg(struct socket *sock, struct msghdr *msg, size_t size)
 	if (err < 0)
 		goto free_skb;
 
+	// SECV: hack! just copy from the placed skb,
+	// TODO: this is not safe, must be done inside inner kernel.
+	//copy_to_user((void*)secure_frame, (void*)skb->data, sizeof(struct canfd_frame));
+	memcpy((void*)secure_frame, (void*)skb->data, sizeof(struct canfd_frame));
+	// this is safe though! can't fake physical addr-to-secure frame mapping if 
+	// if above is done safely. We will simply avoid further switches to&from inner kernel
+	// set flags at the end of this.
+	data = (struct canfd_frame*)skb->data;
+	*((u_int64_t*)&data->data[0]) = secure_phys_addr;
+	//*((u_int64_t*)((struct canfd_frame*)skb->data)->data) = secure_phys_addr;
+
 	err = -EINVAL;
 	if (raw_bad_txframe(ro, skb, dev->mtu))
 		goto free_skb;
diff --git a/security/secv/Kconfig b/security/secv/Kconfig
new file mode 100644
index 000000000000..e3121c96b14f
--- /dev/null
+++ b/security/secv/Kconfig
@@ -0,0 +1,13 @@
+# SECV:
+config SECURITY_SECV_PROCESS_EXIT
+	bool "Process Exit Monitoring LSM by SECV"
+	depends on SECURITY
+	default y
+	help
+	  This LSM module monitors process exit events to support
+      enforcement of SECV policies when processes terminate.
+	  
+	  The module hooks into the task_exit and task_free security
+	  hooks to track process termination.
+	  
+	  If you are unsure how to answer this question, answer N.
\ No newline at end of file
diff --git a/security/secv/exit_lsm.c b/security/secv/exit_lsm.c
new file mode 100644
index 000000000000..d0f8a8b7fbc6
--- /dev/null
+++ b/security/secv/exit_lsm.c
@@ -0,0 +1,52 @@
+/**
+ * SECV Process Exit hook.
+ * This should free the pid mapping in the SSMe.
+ * For security, this should be run in the inner kernel.
+ */
+#include <linux/lsm.h>
+#include <linux/security.h>
+#include <linux/sched.h>
+#include <linux/binfmts.h>
+
+
+#define MODULE_NAME "SECV_exit_lsm"
+
+static void secv_task_exit(struct task_struct* task) 
+{
+    if(!(task->flags & PF_KTHREAD)) {
+        pr_info("%s: Process '%s' (PID: %d, TGID: %d) exiting with code: %d\n",
+        MODULE_NAME,
+        task->comm,
+        task->pid,
+        task->tgid,
+        task->exit_code >> 8);  /* Extract actual exit code */
+    }
+}
+
+static void secv_task_free(struct task_struct* task)
+{
+     pr_debug("%s: Task structure for '%s' (PID: %d) being freed\n",
+             MODULE_NAME, task->comm, task->pid);
+}
+
+/* Define the LSM hooks */
+static struct security_hook_list secv_hooks[] __lsm_ro_after_init = {
+    LSM_HOOK_INIT(task_exit, secv_task_exit),
+    LSM_HOOK_INIT(task_free, secv_task_free),
+};
+
+static int __init secv_task_exit_module_init(void)
+{
+    pr_info("%s: Initializing LSM module\n", MODULE_NAME);
+    
+    /* Register the security hooks */
+    security_add_hooks(secv_hooks, ARRAY_SIZE(secv_hooks), MODULE_NAME);
+    
+    return 0;
+}
+
+/* Required LSM structure */
+DEFINE_LSM(secv_exit_module) = {
+    .name = MODULE_NAME,
+    .init = secv_task_exit_module_init,
+};
\ No newline at end of file
